{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install torch==1.6.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a character-level GPT on some text data\n",
    "\n",
    "The inputs here are simple text files, which we chop up to individual characters and then train GPT on. So you could say this is a char-transformer instead of a char-rnn. Doesn't quite roll off the tongue as well. In this example we will feed it some shakespear, which we'll get it to predict character-level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logging\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    ")\n",
    "minGPT_coef = 8\n",
    "minGPT_coef = min(max(minGPT_coef, 1), 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make deterministic\n",
    "from mingpt.utils import set_seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, block_size):\n",
    "        chars = list(set(data))\n",
    "        data_size, vocab_size = len(data), len(chars)\n",
    "        print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "        \n",
    "        self.stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "        self.itos = { i:ch for i,ch in enumerate(chars) }\n",
    "        self.block_size = block_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.data) / (self.block_size + 1))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # we're actually going to \"cheat\" and pick a spot in the dataset at random\n",
    "        i = np.random.randint(0, len(self.data) - (self.block_size + 1))\n",
    "        chunk = self.data[i:i+self.block_size+1]\n",
    "        dix = [self.stoi[s] for s in chunk]\n",
    "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 128//minGPT_coef # spatial extent of the model for its context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1115394 characters, 65 unique.\n"
     ]
    }
   ],
   "source": [
    "# you can download this file at https://github.com/karpathy/char-rnn/blob/master/data/tinyshakespeare/input.txt\n",
    "text = open('input.txt', 'r').read() # don't worry we won't run out of file handles\n",
    "train_dataset = CharDataset(text, block_size) # one line of poem is roughly 50 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/20/2020 23:51:29 - INFO - mingpt.model -   number of parameters: 5.945600e+04\n"
     ]
    }
   ],
   "source": [
    "from mingpt.model import GPT, GPTConfig\n",
    "mconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size,\n",
    "                  n_layer=8//minGPT_coef, n_head=8//minGPT_coef, n_embd=512//minGPT_coef)\n",
    "model = GPT(\n",
    "    vocab_size=mconf.vocab_size, \n",
    "    block_size=block_size, \n",
    "    n_embd=mconf.n_embd, \n",
    "    n_layer=mconf.n_layer,\n",
    "    n_head=mconf.n_head\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 128: train loss 2.60572. lr 5.999637e-04: 100%|██████████| 129/129 [00:16<00:00,  7.71it/s]\n",
      "epoch 2 iter 128: train loss 2.41788. lr 5.998534e-04: 100%|██████████| 129/129 [00:18<00:00,  7.16it/s]\n",
      "epoch 3 iter 128: train loss 2.34223. lr 5.996691e-04: 100%|██████████| 129/129 [00:18<00:00,  6.98it/s]\n",
      "epoch 4 iter 128: train loss 2.27679. lr 5.994108e-04: 100%|██████████| 129/129 [00:16<00:00,  7.82it/s]\n",
      "epoch 5 iter 128: train loss 2.24310. lr 5.990787e-04: 100%|██████████| 129/129 [00:16<00:00,  7.83it/s]\n",
      "epoch 6 iter 128: train loss 2.24525. lr 5.986728e-04: 100%|██████████| 129/129 [00:16<00:00,  7.97it/s]\n",
      "epoch 7 iter 128: train loss 2.21830. lr 5.981932e-04: 100%|██████████| 129/129 [00:18<00:00,  7.00it/s]\n",
      "epoch 8 iter 128: train loss 2.17025. lr 5.976399e-04: 100%|██████████| 129/129 [00:17<00:00,  7.55it/s]\n",
      "epoch 9 iter 128: train loss 2.19934. lr 5.970133e-04: 100%|██████████| 129/129 [00:22<00:00,  5.81it/s]\n",
      "epoch 10 iter 128: train loss 2.15271. lr 5.963133e-04: 100%|██████████| 129/129 [00:18<00:00,  7.00it/s]\n"
     ]
    }
   ],
   "source": [
    "from mingpt.trainer import Trainer, TrainerConfig\n",
    "\n",
    "# initialize a trainer instance and kick off training\n",
    "tconf = TrainerConfig(max_epochs=10, batch_size=512, learning_rate=6e-4,\n",
    "                      lr_decay=True, warmup_tokens=512*20, final_tokens=200*len(train_dataset)*block_size,\n",
    "                      num_workers=4)\n",
    "trainer = Trainer(model, train_dataset, None, tconf)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O God, O God! Cant of a hing to tand that shy live to mosst my sitesel seet mare wind and ther shold,\n",
      "Tith his the and and thou to the would the witentere trunce me stay, the we the sely my seare withe wor mestay.\n",
      "\n",
      "Marce.\n",
      "\n",
      "COMINTENTES:\n",
      "As alt werstice heare, thin selvioughs say my to sir shat,\n",
      "Whather ang my the hereace she herst a mor hat shoul sere to me and to me seay arthe and my les arient, wore thy, thost,\n",
      "Whath sin a with say ther'd,\n",
      "When to wan that that that to herem ther thilll thear him artince ancone and, me man,\n",
      "Whathou desen may.\n",
      "\n",
      "KING RESTO:\n",
      "How'll and\n",
      "A heard have with that me,\n",
      "And ther and this the and he here have wer sel and she how stre tiones thing me me wom thy my the so din somon:\n",
      "A then and the my that the have wom sher'st thee with won say thou me think that and say, murdentlen mentent man:\n",
      "The monted, my sought, there me with say hou din and\n",
      "The me the me,\n",
      "That ath his to have thou that to day,\n",
      "Wher thour hearther mone man:\n",
      "Harth, man, shing alll to mary have sto me my shimp of thour sher a the man:\n",
      "I thater that so me,\n",
      "To haven that hy mourn the seak ther a me, whathe seer my me seare as thance and say, that as to sen al to shomently to mane,\n",
      "And the to hung, wheeard, that warst thou say, sare with to the seall some hard ting arth him strue ast han hou shall thy live the hopare sourd say may theer hou sen mant,\n",
      "Wil my like a muse,\n",
      "Which to did that and to mus that the man there my sof my thou dides,\n",
      "Thon her, thou seee hart, shy mus thand to thy me have me seake thy liven mis and sentle, wath me and that a stome,\n",
      "Thave tall wis, so the havers on me man there ant to therse ting, my and and he with will that there with well the my man to he what thy, thy my let thy,\n",
      "I with my there then a to derst thath shal to sand mading,\n",
      "A so the he the the mad me hou have, sand thy how, seen and this to heard, to hum wom an man: I ther have why hus,\n",
      "I an the may stay a will ther's are,\n",
      "And seake and my to so to the mosen the tan sall to dis on to say his sent trunce:\n"
     ]
    }
   ],
   "source": [
    "# alright, let's sample some character-level shakespear\n",
    "from mingpt.utils import sample\n",
    "\n",
    "context = \"O God, O God!\"\n",
    "x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None,...].to(trainer.device)\n",
    "y = sample(model, x, 2000, temperature=0.9, sample=True, top_k=5)[0]\n",
    "completion = ''.join([train_dataset.itos[int(i)] for i in y])\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# well that was fun"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
