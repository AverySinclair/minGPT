{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cordless-auditor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logging\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "matched-equation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make deterministic\n",
    "from mingpt.utils import set_seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "varying-atlas",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from collections import OrderedDict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "historic-thermal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Shakespeare texts from URL. There are original texts and translated to modern English\n",
    "text_modern = urllib.request.urlopen('https://raw.githubusercontent.com/emukans/shakespeare-texts/master/all_modern.txt').read().decode(\"utf-8\", \"ignore\")\n",
    "text_original = urllib.request.urlopen('https://raw.githubusercontent.com/emukans/shakespeare-texts/master/all_original.txt').read().decode(\"utf-8\", \"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "boxed-charm",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, data, vocab_size):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.vocab = self.build_vocab(data)\n",
    "        \n",
    "        self.stoi = { ch:i for i,ch in enumerate(self.vocab) }\n",
    "        self.itos = { i:ch for i,ch in enumerate(self.vocab) }\n",
    "    \n",
    "    def sort_vocab(self, vocab):\n",
    "        \"\"\"\n",
    "        Vocab should have the followind order: hashtag, numbers, characters sorted by length.\n",
    "        Hashtags should go first, because they will be used as dividers on tokenization step.\n",
    "        Numbers should go before characters, because token ids are numbers. Otherwise token ids will be considered as usual numbers and replaced twice.\n",
    "        \"\"\"\n",
    "        sorted_vocab = sorted(vocab, key=lambda x: len(x), reverse=True)\n",
    "        tag = [int(s) for s in sorted_vocab if s == '#']\n",
    "        \n",
    "        numeric = [int(s) for s in sorted_vocab if s.isnumeric()]\n",
    "        numeric = [str(s) for s in sorted(numeric, reverse=True)]\n",
    "        rest = [s for s in sorted_vocab if not s.isnumeric()]\n",
    "        \n",
    "        sorted_vocab = tag + numeric + rest\n",
    "        \n",
    "        return sorted_vocab\n",
    "    \n",
    "    def build_vocab(self, data):\n",
    "        \"\"\"\n",
    "        Build vocabluary using BPE alghorithm.\n",
    "        \"\"\"\n",
    "        vocab = set(data)\n",
    "        if len(vocab) > self.vocab_size:\n",
    "            raise ValueError('Vocab size should be greater than unique char count')\n",
    "\n",
    "        # check all available characters\n",
    "        char_set = {c for c in vocab if c.isalpha()}\n",
    "        \n",
    "        # candidates dictionary will contain a set of all available tokens to search\n",
    "        candidate_dict = dict().fromkeys(char_set, 0)\n",
    "        \n",
    "        # occurrences will contain all matched tokens and the count, how many times the token has been found.\n",
    "        token_occurrences = OrderedDict()\n",
    "        while len(vocab) < self.vocab_size:\n",
    "            for candidate in candidate_dict.keys():\n",
    "                occurrences = data.count(candidate)\n",
    "                candidate_dict[candidate] = occurrences\n",
    "\n",
    "            candidate_dict = {candidate: count for candidate, count in candidate_dict.items() if count}\n",
    "            vocab.update(set(candidate_dict.keys()))\n",
    "            token_occurrences.update(candidate_dict)\n",
    "\n",
    "            # build new candidates\n",
    "            temp_candidate_set = set()\n",
    "            for char in char_set:\n",
    "                # don't test candidates with occurency <= 2. New candidates won't have occurency higher than 2\n",
    "                temp_candidate_set.update({candidate + char for candidate in candidate_dict.keys() if token_occurrences[candidate] > 2})\n",
    "\n",
    "            candidate_dict = dict().fromkeys(temp_candidate_set, 0)\n",
    "\n",
    "        tokens_to_remove = len(vocab) - self.vocab_size\n",
    "        token_occurrences = OrderedDict(sorted(token_occurrences.items(), key=lambda x: x[1], reverse=True))\n",
    "        for _ in range(tokens_to_remove):\n",
    "            token, _ = token_occurrences.popitem()\n",
    "            vocab.remove(token)\n",
    "\n",
    "        sorted_vocab = self.sort_vocab(vocab)\n",
    "        \n",
    "        # add a special token for unknown tokens\n",
    "        sorted_vocab.append('<unk>')\n",
    "        self.vocab_size += 1 # plus <unk> special token\n",
    "        \n",
    "        return sorted_vocab\n",
    "    \n",
    "    def tokenize(self, data, block_size):\n",
    "        for token in self.vocab:\n",
    "            data = data.replace(token, f'#{self.stoi[token]}#')\n",
    "\n",
    "        # If everything went well, first and last characters won't have # pair. Need to trim them\n",
    "        data = data[1:-1]\n",
    "        # Split by ## pairs\n",
    "        tokenized_text = data.split('##')\n",
    "        # Filter empty strings\n",
    "        tokenized_text = [x for x in tokenized_text if x]\n",
    "        result = []\n",
    "        for tokenized in tokenized_text:\n",
    "            # In case other single # found, replace them with <unk> special token, marking the element as unknown\n",
    "            if '#' in tokenized:\n",
    "                for unknown_candidate in tokenized.split('#'):\n",
    "                    if unknown_candidate.isnumeric():\n",
    "                        result.append(self.itos[int(unknown_candidate)])\n",
    "                    else:\n",
    "                        result.append('<unk>')\n",
    "            else:\n",
    "                result.append(self.itos[int(tokenized)])\n",
    "\n",
    "        # all texts should have equal size. We can make text length equal by filling text with spaces\n",
    "        for _ in range(block_size - len(result)):\n",
    "            result.append(' ')\n",
    "            \n",
    "        # in case the sentence is longer, than block_size, we trim the sentence\n",
    "        return result[:block_size]\n",
    "    \n",
    "    def encode(self, data):\n",
    "        return [self.stoi[s] for s in data]\n",
    "    \n",
    "    def decode(self, data):\n",
    "        return ''.join([self.itos[int(i)] for i in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "advised-shelter",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "\n",
    "# building vocabluary can take some time. ~5 minutes for 10_000 tokens for each tokenizer. \n",
    "tokenizer_modern = Tokenizer(text_modern, vocab_size)\n",
    "tokenizer_original = Tokenizer(text_original, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "significant-animal",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class WordDataset(Dataset):\n",
    "\n",
    "    def __init__(self, original, modern, tokenizer_original, tokenizer_modern, block_size):\n",
    "        self.tokenizer_original = tokenizer_original\n",
    "        self.tokenizer_modern = tokenizer_modern\n",
    "        \n",
    "        self.block_size = block_size * 2\n",
    "        self.original = [tokenizer_original.tokenize(t, block_size) for t in original]\n",
    "        self.modern = [tokenizer_modern.tokenize(t, block_size) for t in modern]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.original)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        The idea is to get a sentence in a modern English\n",
    "        and translate it to Shakespeare English.\n",
    "        \n",
    "        In the init method we already split a sentence into tokens and filled with spaces,\n",
    "        to have an equal sentence size. In this method we just encode the tokens to\n",
    "        ids (a list of numbers), and we're trying to map ids sequences\n",
    "        (original Englisn and modern English)\n",
    "        \"\"\"\n",
    "        \n",
    "        modern_text = self.tokenizer_modern.encode(self.modern[idx])\n",
    "        original_text = self.tokenizer_original.encode(self.original[idx])\n",
    "        dix = modern_text + original_text\n",
    "        \n",
    "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
    "        y[:int(self.block_size / 2) - 1] = -100\n",
    "        \n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "emotional-metadata",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle texts by lines\n",
    "texts = list(zip(text_modern.splitlines(), text_original.splitlines()))\n",
    "random.shuffle(texts)\n",
    "\n",
    "text_modern, text_original = zip(*texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "floral-ridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split texts into train, test and validation datasets\n",
    "train_dataset_size = round(0.85 * len(text_modern))\n",
    "test_dataset_size = round(0.1 * len(text_modern))\n",
    "valid_dataset_size = round(0.05 * len(text_modern))\n",
    "\n",
    "train_modern = text_modern[:train_dataset_size]\n",
    "test_modern = text_modern[train_dataset_size:train_dataset_size + test_dataset_size]\n",
    "valid_modern = text_modern[-valid_dataset_size:]\n",
    "\n",
    "train_original = text_original[:train_dataset_size]\n",
    "test_original = text_original[train_dataset_size:train_dataset_size + test_dataset_size]\n",
    "valid_original = text_original[-valid_dataset_size:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fitted-resident",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 100  # the estimate how long lines the text could be (token count)\n",
    "\n",
    "train_dataset = WordDataset(train_original, train_modern, tokenizer_original, tokenizer_modern, block_size)\n",
    "test_dataset = WordDataset(test_original, test_modern, tokenizer_original, tokenizer_modern, block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ruled-astronomy",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/01/2021 23:56:40 - INFO - mingpt.model -   number of parameters: 1.664922e+07\n"
     ]
    }
   ],
   "source": [
    "from mingpt.model import GPT, GPTConfig\n",
    "mconf = GPTConfig(tokenizer_original.vocab_size, train_dataset.block_size,\n",
    "                  n_layer=2, n_head=4, n_embd=512)\n",
    "model = GPT(mconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "anonymous-hepatitis",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 417: train loss 0.56379. lr 3.000000e-04: 100%|██████████| 418/418 [01:58<00:00,  3.54it/s]\n",
      "04/02/2021 00:03:13 - INFO - mingpt.trainer -   test loss: 0.626109\n",
      "epoch 2 iter 417: train loss 0.48679. lr 2.979542e-04: 100%|██████████| 418/418 [01:58<00:00,  3.52it/s]\n",
      "04/02/2021 00:05:17 - INFO - mingpt.trainer -   test loss: 0.571073\n",
      "epoch 3 iter 417: train loss 0.41815. lr 2.918726e-04: 100%|██████████| 418/418 [01:58<00:00,  3.52it/s]\n",
      "04/02/2021 00:07:20 - INFO - mingpt.trainer -   test loss: 0.548723\n",
      "epoch 4 iter 417: train loss 0.46000. lr 2.819211e-04: 100%|██████████| 418/418 [01:58<00:00,  3.53it/s]\n",
      "04/02/2021 00:09:23 - INFO - mingpt.trainer -   test loss: 0.531342\n",
      "epoch 5 iter 417: train loss 0.37339. lr 2.683711e-04: 100%|██████████| 418/418 [01:58<00:00,  3.52it/s]\n",
      "04/02/2021 00:11:26 - INFO - mingpt.trainer -   test loss: 0.518601\n",
      "epoch 6 iter 417: train loss 0.39091. lr 2.515922e-04: 100%|██████████| 418/418 [01:58<00:00,  3.52it/s]\n",
      "04/02/2021 00:13:30 - INFO - mingpt.trainer -   test loss: 0.516440\n",
      "epoch 7 iter 417: train loss 0.51425. lr 2.320422e-04: 100%|██████████| 418/418 [01:58<00:00,  3.53it/s]\n",
      "04/02/2021 00:15:32 - INFO - mingpt.trainer -   test loss: 0.508544\n",
      "epoch 8 iter 417: train loss 0.35870. lr 2.102543e-04: 100%|██████████| 418/418 [01:57<00:00,  3.56it/s]\n",
      "04/02/2021 00:17:35 - INFO - mingpt.trainer -   test loss: 0.510203\n",
      "epoch 9 iter 417: train loss 0.30268. lr 1.868228e-04: 100%|██████████| 418/418 [01:57<00:00,  3.56it/s]\n",
      "04/02/2021 00:19:37 - INFO - mingpt.trainer -   test loss: 0.516476\n",
      "epoch 10 iter 417: train loss 0.23884. lr 1.623869e-04: 100%|██████████| 418/418 [01:57<00:00,  3.56it/s]\n",
      "04/02/2021 00:21:39 - INFO - mingpt.trainer -   test loss: 0.525756\n",
      "epoch 11 iter 417: train loss 0.27640. lr 1.376131e-04: 100%|██████████| 418/418 [01:57<00:00,  3.56it/s]\n",
      "04/02/2021 00:23:41 - INFO - mingpt.trainer -   test loss: 0.527686\n",
      "epoch 12 iter 417: train loss 0.28511. lr 1.131772e-04: 100%|██████████| 418/418 [01:57<00:00,  3.56it/s]\n",
      "04/02/2021 00:25:43 - INFO - mingpt.trainer -   test loss: 0.530774\n",
      "epoch 13 iter 417: train loss 0.22887. lr 8.974569e-05: 100%|██████████| 418/418 [01:57<00:00,  3.56it/s]\n",
      "04/02/2021 00:27:45 - INFO - mingpt.trainer -   test loss: 0.536318\n",
      "epoch 14 iter 417: train loss 0.19394. lr 6.795778e-05: 100%|██████████| 418/418 [01:58<00:00,  3.52it/s]\n",
      "04/02/2021 00:29:49 - INFO - mingpt.trainer -   test loss: 0.543161\n",
      "epoch 15 iter 417: train loss 0.19608. lr 4.840776e-05: 100%|██████████| 418/418 [01:58<00:00,  3.52it/s]\n",
      "04/02/2021 00:31:52 - INFO - mingpt.trainer -   test loss: 0.552905\n",
      "epoch 16 iter 417: train loss 0.19222. lr 3.162892e-05: 100%|██████████| 418/418 [01:58<00:00,  3.53it/s]\n",
      "04/02/2021 00:33:55 - INFO - mingpt.trainer -   test loss: 0.552591\n",
      "epoch 17 iter 417: train loss 0.15980. lr 3.000000e-05: 100%|██████████| 418/418 [01:57<00:00,  3.56it/s]\n",
      "04/02/2021 00:35:57 - INFO - mingpt.trainer -   test loss: 0.559179\n",
      "epoch 18 iter 417: train loss 0.16607. lr 3.000000e-05: 100%|██████████| 418/418 [01:57<00:00,  3.56it/s]\n",
      "04/02/2021 00:37:59 - INFO - mingpt.trainer -   test loss: 0.563958\n",
      "epoch 19 iter 417: train loss 0.15340. lr 3.000000e-05: 100%|██████████| 418/418 [01:57<00:00,  3.56it/s]\n",
      "04/02/2021 00:40:02 - INFO - mingpt.trainer -   test loss: 0.572551\n",
      "epoch 20 iter 417: train loss 0.15097. lr 3.000000e-05: 100%|██████████| 418/418 [01:57<00:00,  3.56it/s]\n",
      "04/02/2021 00:42:04 - INFO - mingpt.trainer -   test loss: 0.565438\n"
     ]
    }
   ],
   "source": [
    "from mingpt.trainer import Trainer, TrainerConfig\n",
    "\n",
    "tokens_per_epoch = len(train_dataset) * block_size\n",
    "train_epochs = 20\n",
    "\n",
    "# initialize a trainer instance and kick off training\n",
    "tconf = TrainerConfig(max_epochs=train_epochs, batch_size=64, learning_rate=3e-4,\n",
    "                      lr_decay=True, warmup_tokens=tokens_per_epoch, final_tokens=train_epochs*tokens_per_epoch,\n",
    "                      num_workers=2)\n",
    "trainer = Trainer(model, train_dataset, test_dataset, tconf)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "greek-travel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modern:             I knew your father.\n",
      "Predicted original: I knew your father.                                                                                           \n",
      "Real original:      I knew your father.\n",
      "--------------------------------------------------\n",
      "Modern:             It will be out, it will be out.\n",
      "Predicted original: Twill be out, and be out of will.                                                                                  \n",
      "Real original:      Twill out, 'twill.\n",
      "--------------------------------------------------\n",
      "Modern:             What’s going on these days?\n",
      "Predicted original: How now?                                                                                                \n",
      "Real original:      Is ’t come to this?\n",
      "--------------------------------------------------\n",
      "Modern:             Early in the morning I’ll go visit Desdemona and plead my case.\n",
      "Predicted original: As Early the morn-day will, The faith of h and perchance to my mistress.                                                                \n",
      "Real original:      I think it freely, and betimes in the morning I will beseech the virtuous Desdemona to undertake for me.\n",
      "--------------------------------------------------\n",
      "Modern:             Oh, really?\n",
      "Predicted original: Indeed, i' faith?                                                                                          \n",
      "Real original:      Indeed?\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# alright, let's translate some modern English to Shakespeare\n",
    "from mingpt.utils import sample\n",
    "from random import choice\n",
    "\n",
    "for _ in range(5):\n",
    "    idx = choice(range(len(valid_original)))\n",
    "\n",
    "    context = valid_modern[idx]\n",
    "    x = torch.tensor(tokenizer_modern.encode(tokenizer_modern.tokenize(context, block_size)), dtype=torch.long)[None,...].to(trainer.device)\n",
    "    y = sample(model, x, block_size, temperature=1.0, sample=True, top_k=10)[0]\n",
    "\n",
    "    predicted = y[block_size:]\n",
    "    completion = tokenizer_original.decode(predicted)\n",
    "    print(f'Modern:             {context}')\n",
    "    print(f'Predicted original: {completion}')\n",
    "    print(f'Real original:      {valid_original[idx]}')\n",
    "    print('--------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minimal-carter",
   "metadata": {},
   "source": [
    "Well, the translation isn't perfect. It's more like a mix of modern and Shakespeare English. To solve it, need more data, a bigger model, use a pre-trained language model, and fine-tune it on Shakespeare texts.\n",
    "\n",
    "Nevertheless, the results above are from the validation dataset, which didn't participate in training and the words are more or less real."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
