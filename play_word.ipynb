{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "atomic-option",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logging\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "addressed-brother",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make deterministic\n",
    "from mingpt.utils import set_seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "twenty-shift",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from collections import OrderedDict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "painted-halifax",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Shakespeare texts from URL. Texts are translated to modern English\n",
    "text = urllib.request.urlopen('https://raw.githubusercontent.com/emukans/shakespeare-texts/master/all_modern.txt').read().decode(\"utf-8\", \"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "above-championship",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, data, vocab_size):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.vocab = self.build_vocab(data)\n",
    "        \n",
    "        self.stoi = { ch:i for i,ch in enumerate(self.vocab) }\n",
    "        self.itos = { i:ch for i,ch in enumerate(self.vocab) }\n",
    "    \n",
    "    def sort_vocab(self, vocab):\n",
    "        \"\"\"\n",
    "        Vocab should have the followind order: hashtag, numbers, characters sorted by length.\n",
    "        Hashtags should go first, because they will be used as dividers on tokenization step.\n",
    "        Numbers should go before characters, because token ids are numbers. Otherwise token ids will be considered as usual numbers and replaced twice.\n",
    "        \"\"\"\n",
    "        sorted_vocab = sorted(vocab, key=lambda x: len(x), reverse=True)\n",
    "        tag = [int(s) for s in sorted_vocab if s == '#']\n",
    "        \n",
    "        numeric = [int(s) for s in sorted_vocab if s.isnumeric()]\n",
    "        numeric = [str(s) for s in sorted(numeric, reverse=True)]\n",
    "        rest = [s for s in sorted_vocab if not s.isnumeric()]\n",
    "        \n",
    "        sorted_vocab = tag + numeric + rest\n",
    "        \n",
    "        return sorted_vocab\n",
    "    \n",
    "    def build_vocab(self, data):\n",
    "        \"\"\"\n",
    "        Build vocabluary using BPE alghorithm.\n",
    "        \"\"\"\n",
    "        vocab = set(data)\n",
    "        if len(vocab) > self.vocab_size:\n",
    "            raise ValueError('Vocab size should be greater than unique char count')\n",
    "\n",
    "        # check all available characters\n",
    "        char_set = {c for c in vocab if c.isalpha()}\n",
    "        \n",
    "        # candidates dictionary will contain a set of all available tokens to search\n",
    "        candidate_dict = dict().fromkeys(char_set, 0)\n",
    "        \n",
    "        # occurrences will contain all matched tokens and the count, how many times the token has been found.\n",
    "        token_occurrences = OrderedDict()\n",
    "        while len(vocab) < self.vocab_size:\n",
    "            for candidate in candidate_dict.keys():\n",
    "                occurrences = data.count(candidate)\n",
    "                candidate_dict[candidate] = occurrences\n",
    "\n",
    "            candidate_dict = {candidate: count for candidate, count in candidate_dict.items() if count}\n",
    "            vocab.update(set(candidate_dict.keys()))\n",
    "            token_occurrences.update(candidate_dict)\n",
    "\n",
    "            # build new candidates\n",
    "            temp_candidate_set = set()\n",
    "            for char in char_set:\n",
    "                # don't test candidates with occurency <= 2. New candidates won't have occurency higher than 2\n",
    "                temp_candidate_set.update({candidate + char for candidate in candidate_dict.keys() if token_occurrences[candidate] > 2})\n",
    "\n",
    "            candidate_dict = dict().fromkeys(temp_candidate_set, 0)\n",
    "\n",
    "        tokens_to_remove = len(vocab) - self.vocab_size\n",
    "        token_occurrences = OrderedDict(sorted(token_occurrences.items(), key=lambda x: x[1], reverse=True))\n",
    "        for _ in range(tokens_to_remove):\n",
    "            token, _ = token_occurrences.popitem()\n",
    "            vocab.remove(token)\n",
    "\n",
    "        sorted_vocab = self.sort_vocab(vocab)\n",
    "        \n",
    "        # add a special token for unknown tokens\n",
    "        sorted_vocab.append('<unk>')\n",
    "        self.vocab_size += 1 # plus <unk> special token\n",
    "        \n",
    "        return sorted_vocab\n",
    "    \n",
    "    def tokenize(self, data):\n",
    "        for token in self.vocab:\n",
    "            data = data.replace(token, f'#{self.stoi[token]}#')\n",
    "\n",
    "        # If everything went well, first and last characters won't have # pair. Need to trim them\n",
    "        data = data[1:-1]\n",
    "        # Split by ## pairs\n",
    "        tokenized_text = data.split('##')\n",
    "        # Filter empty strings\n",
    "        tokenized_text = [x for x in tokenized_text if x]\n",
    "        result = []\n",
    "        for tokenized in tokenized_text:\n",
    "            # In case other single # found, replace them with <unk> special token, marking the element as unknown\n",
    "            if '#' in tokenized:\n",
    "                for unknown_candidate in tokenized.split('#'):\n",
    "                    if unknown_candidate.isnumeric():\n",
    "                        result.append(self.itos[int(unknown_candidate)])\n",
    "                    else:\n",
    "                        result.append('<unk>')\n",
    "            else:\n",
    "                result.append(self.itos[int(tokenized)])\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def encode(self, data):\n",
    "        return [self.stoi[s] for s in data]\n",
    "    \n",
    "    def decode(self, data):\n",
    "        return ''.join([self.itos[int(i)] for i in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "designed-enterprise",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "\n",
    "# building vocabluary can take some time. ~5 minutes for 10_000 tokens. \n",
    "tokenizer = Tokenizer(text, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "extended-kruger",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class WordDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, block_size, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.block_size = block_size\n",
    "        #self.vocab_size = self.tokenizer.vocab_size\n",
    "        self.data = self.tokenizer.tokenize(data)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # grab a chunk of (block_size + 1) characters from the data\n",
    "        chunk = self.data[idx:idx + self.block_size + 1]\n",
    "        # encode every character to an integer\n",
    "        dix = self.tokenizer.encode(chunk)\n",
    "        \"\"\"\n",
    "        arrange data and targets so that the first i elements of x\n",
    "        will be asked to predict the i-th element of y. Notice that\n",
    "        the eventual language model will actually make block_size\n",
    "        individual predictions at the same time based on this data,\n",
    "        so we are being clever and amortizing the cost of the forward\n",
    "        pass of the network. So for example if block_size is 4, then\n",
    "        we could e.g. sample a chunk of text \"hello\", the integers in\n",
    "        x will correspond to \"hell\" and in y will be \"ello\". This will\n",
    "        then actually \"multitask\" 4 separate examples at the same time\n",
    "        in the language model:\n",
    "        - given just \"h\", please predict \"e\" as next\n",
    "        - given \"he\" please predict \"l\" next\n",
    "        - given \"hel\" predict \"l\" next\n",
    "        - given \"hell\" predict \"o\" next\n",
    "        \n",
    "        In addition, because the DataLoader will create batches of examples,\n",
    "        every forward/backward pass during traning will simultaneously train\n",
    "        a LOT of predictions, amortizing a lot of computation. In particular,\n",
    "        for a batched input of integers X (B, T) where B is batch size and\n",
    "        T is block_size and Y (B, T), the network will during training be\n",
    "        simultaneously training to make B*T predictions, all at once! Of course,\n",
    "        at test time we can paralellize across batch B, but unlike during training\n",
    "        we cannot parallelize across the time dimension T - we have to run\n",
    "        a forward pass of the network to recover the next single character of the \n",
    "        sequence along each batch dimension, and repeatedly always feed in a next\n",
    "        character to get the next one.\n",
    "        \n",
    "        So yes there is a big asymmetry between train/test time of autoregressive\n",
    "        models. During training we can go B*T at a time with every forward pass,\n",
    "        but during test time we can only go B at a time, T times, with T forward \n",
    "        passes.\n",
    "        \"\"\"\n",
    "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "hollywood-deviation",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 50 # spatial extent of the model for its context\n",
    "\n",
    "train_dataset = WordDataset(text, block_size, tokenizer) # one line of poem is roughly 50 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "operational-tourist",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/30/2021 16:01:04 - INFO - mingpt.model -   number of parameters: 3.548672e+07\n"
     ]
    }
   ],
   "source": [
    "from mingpt.model import GPT, GPTConfig\n",
    "mconf = GPTConfig(train_dataset.tokenizer.vocab_size, train_dataset.block_size,\n",
    "                  n_layer=8, n_head=8, n_embd=512)\n",
    "model = GPT(mconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "prostate-redhead",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 3260: train loss 0.55002. lr 3.000578e-04: 100%|██████████| 3261/3261 [40:39<00:00,  1.34it/s]\n",
      "epoch 2 iter 3260: train loss 0.33295. lr 6.000000e-05: 100%|██████████| 3261/3261 [40:49<00:00,  1.33it/s]\n"
     ]
    }
   ],
   "source": [
    "from mingpt.trainer import Trainer, TrainerConfig\n",
    "\n",
    "# initialize a trainer instance and kick off training\n",
    "tconf = TrainerConfig(max_epochs=2, batch_size=256, learning_rate=6e-4,\n",
    "                      lr_decay=True, warmup_tokens=512*20, final_tokens=2*len(train_dataset)*block_size,\n",
    "                      num_workers=2)\n",
    "trainer = Trainer(model, train_dataset, None, tconf)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "arctic-devil",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O God, O God!\n",
      "O church, O world Now lies here, and then there is the first time he plays his part.\n",
      "Welcome.\n",
      "Set the honorable old man down and let him eat.\n",
      "I thank you very much on his behalf.\n",
      "Welcome.\n",
      "Eat.\n",
      "I won’t trouble you yet with questions about your situation.—Some music, please, and, good friend, sing.\n",
      "Time’s not the one in debt.\n",
      "Your logic is so foolish.\n",
      "Take it to your master and bring him home immediately.\n",
      "Every person I meet greets me like an old friend, and every one of them knows my name.\n",
      "Some of them give me money, some invite me places, some thank me for the kind things I’ve done for them, some try to sell me things.\n",
      "Just now a tailor showed me fabrics he bought especially for me and then started to take my measurements.\n",
      "These are tricks of the imagination, and this place is filled with magicians.\n",
      "Here’s the money you wanted, master.\n",
      "Who’s this Adam you speak of?\n",
      "Not the Adam from the garden of Eden, but the Adam from the jailhouse.\n",
      "I don’t know what you’re talking about.\n",
      "No?\n",
      "You mean an officer?\n",
      "Yes, the leader of the team; the one that gets you if you can’t pay a debt; the one who assumes people are always going to bed and says to them, “Have arrest.” Well, sir, stop your joking there.\n",
      "Are any ships leaving tonight?\n",
      "Can we go?\n",
      "Why, sir, I told you an hour ago that the good ship This fellow’s gone mad, and so have I. We’re in some kind of dream world.\n",
      "Please, somebody, get us out of here!\n",
      "Good to see you, Master Antipholus.\n",
      "I see you’ve met with the jeweler.\n",
      "Is that chain you’re wearing the one you promised to give to me?\n",
      "Get away from me, Satan!\n",
      "Don’t try to tempt me!\n",
      "Master, is this Satan’s mistress?\n",
      "She’s the devil.\n",
      "No, she’s worse: she’s the devil’s mother, and she comes to us disguised as an easy wench.\n",
      "But fire also gives off light, and fire will burn you.\n",
      "You and your servant are very funny, sir.\n",
      "Will you come with me?\n",
      "Can we finish our lunch?\n",
      "Master, if you eat with her, bring really long silverware.\n",
      "Why, Dromio?\n",
      "Some  Now listen, either give me my ring or give me the necklace.\n",
      "I hope you’re not trying to cheat me.\n",
      "Come, Dromio, let’s go.\n",
      "Mistress, you know about that.\n",
      "Antipholus has gone insane, no question about it.\n",
      "He has a ring of mine, worth forty ducats, and he promised to give me a necklace in exchange for it.\n",
      "Now he won’t give me either.\n",
      "The reason I think he’s insane, besides the way he just acted, is that he told a senseless story over lunch about being locked out of his own house.\n",
      "His wife probably did it on purpose because she knows what kind of fits he’s having.\n",
      "I must go to his house and tell his wife that he came bursting into my place like a lunatic and stole my ring.\n",
      "It’s my best option: I can’t afford to lose forty ducats.\n",
      "I’ve spent five years in the remotest parts of Greece and roaming all over Asia.\n",
      "However, I’ll do what I can for you.\n",
      "I will allow you one day to look for help in Ephesus.\n",
      "I will.\n",
      "Don’t worry, man, I won’t try to escape.\n",
      "I tell you, this will make her angry when she hears about it.\n",
      "Here’s my servant.\n",
      "I think he’s got the money.\n",
      "Hello there, sir!\n",
      "Do you have what I told you to get?\n",
      "Yes.\n",
      "But where’s the money?\n",
      "Why, sir, I spent it on this rope.\n",
      "I can get you five hundred ropes for that price.\n",
      "Why did I just send you home?\n",
      "And here I am, with that purse I am here.\n",
      "What is do you need?\n",
      "This is the matter, Nurse, leave us alone a while, We must talk in secret.\n",
      "nurse, come back again; I have remembered, you can hear our conversation.\n",
      "You know my daughter's at a pretty age.\n",
      "Believe me, I can tell her age to the hour.\n",
      "She's not fourteen.\n",
      "I'll bet fourteen of my teeth, And yet, to tell the truth, I’ve only got four, She is not fourteen.\n",
      "How long is it now To the middle of July?\n",
      "Two weeks and a couple of days.\n",
      "Couple of days or not, of all days in the year, Come the middle of July, she’ll be fourteen.\n",
      "My daughter, Susan, and she God \n"
     ]
    }
   ],
   "source": [
    "# alright, let's sample some word-level Shakespeare\n",
    "from mingpt.utils import sample\n",
    "\n",
    "context = \"O God, O God!\"\n",
    "x = torch.tensor(tokenizer.encode(context), dtype=torch.long)[None,...].to(trainer.device)\n",
    "y = sample(model, x, 2000, temperature=1.0, sample=True, top_k=10)[0]\n",
    "completion = tokenizer.decode(y)\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confidential-domain",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
