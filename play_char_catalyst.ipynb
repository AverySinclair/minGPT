{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install catalyst==20.08.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a character-level GPT on some text data\n",
    "\n",
    "The inputs here are simple text files, which we chop up to individual characters and then train GPT on. So you could say this is a char-transformer instead of a char-rnn. Doesn't quite roll off the tongue as well. In this example we will feed it some shakespear, which we'll get it to predict character-level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "minGPT_coef = 8\n",
    "minGPT_coef = min(max(minGPT_coef, 1), 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make deterministic\n",
    "from catalyst import utils\n",
    "utils.set_global_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, block_size):\n",
    "        chars = list(set(data))\n",
    "        data_size, vocab_size = len(data), len(chars)\n",
    "        print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "        \n",
    "        self.stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "        self.itos = { i:ch for i,ch in enumerate(chars) }\n",
    "        self.block_size = block_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.data) / (self.block_size + 1))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # we're actually going to \"cheat\" and pick a spot in the dataset at random\n",
    "        i = np.random.randint(0, len(self.data) - (self.block_size + 1))\n",
    "        chunk = self.data[i:i+self.block_size+1]\n",
    "        dix = [self.stoi[s] for s in chunk]\n",
    "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 128//minGPT_coef # spatial extent of the model for its context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! wget https://github.com/karpathy/char-rnn/blob/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1115394 characters, 65 unique.\n"
     ]
    }
   ],
   "source": [
    "# you can download this file at https://github.com/karpathy/char-rnn/blob/master/data/tinyshakespeare/input.txt\n",
    "text = open('input.txt', 'r').read() # don't worry we won't run out of file handles\n",
    "train_dataset = CharDataset(text, block_size) # one line of poem is roughly 50 characters\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mingpt.model import GPT\n",
    "from mingpt.utils import prepare_optimizer\n",
    "\n",
    "model = GPT(\n",
    "    vocab_size=train_dataset.vocab_size, \n",
    "    block_size=block_size, \n",
    "    n_embd=512//minGPT_coef, \n",
    "    n_layer=8//minGPT_coef,\n",
    "    n_head=8//minGPT_coef\n",
    ")\n",
    "optimizer = prepare_optimizer(\n",
    "    model=model, \n",
    "    learning_rate=6e-4, \n",
    "    weight_decay=0.1, \n",
    "    betas=(0.9, 0.95)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from catalyst.core import CallbackOrder, CallbackNode\n",
    "from catalyst import dl\n",
    "\n",
    "\n",
    "class CustomRunner(dl.Runner):\n",
    "    \n",
    "    def _handle_batch(self, batch):\n",
    "        x, targets = batch\n",
    "        logits = self.model(x)\n",
    "        loss = F.cross_entropy(\n",
    "            logits.view(-1, logits.size(-1)), targets.view(-1)\n",
    "        )\n",
    "        self.input = batch\n",
    "        self.output = logits\n",
    "        self.batch_metrics.update(**{\"loss\": loss})\n",
    "        \n",
    "        if self.is_train_loader:\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "\n",
    "class CustomCallback(dl.Callback):\n",
    "\n",
    "    def __init__(self, learning_rate, warmup_tokens, final_tokens, lr_decay=True):\n",
    "        super().__init__(order=CallbackOrder.scheduler, node=CallbackNode.all)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tokens = 0\n",
    "        self.final_tokens = final_tokens\n",
    "        self.lr_decay = lr_decay\n",
    "        self.warmup_tokens = warmup_tokens\n",
    "\n",
    "    def on_batch_end(self, runner):\n",
    "        if not runner.is_train_loader:\n",
    "            return\n",
    "        optimizer = runner.optimizer\n",
    "        x, y = runner.input\n",
    "\n",
    "        if self.lr_decay:\n",
    "            self.tokens += (y >= 0).sum()  # number of tokens processed this step (i.e. label is not -100)\n",
    "            if self.tokens < self.warmup_tokens:\n",
    "                # linear warmup\n",
    "                lr_mult = float(self.tokens) / float(max(1, self.warmup_tokens))\n",
    "            else:\n",
    "                # cosine learning rate decay\n",
    "                progress = float(self.tokens - self.warmup_tokens) / float(\n",
    "                    max(1, self.final_tokens - self.warmup_tokens))\n",
    "                lr_mult = max(0.1, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
    "            lr = self.learning_rate * lr_mult\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "\n",
    "\n",
    "runner = CustomRunner()\n",
    "callbacks = [\n",
    "    CustomCallback(\n",
    "        learning_rate=6e-4,\n",
    "        warmup_tokens=512*20, \n",
    "        final_tokens=200*len(train_dataset)*block_size,\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/scitator/Documents/git/catalyst.release/catalyst/dl/experiment/experiment.py:190: UserWarning:\n",
      "\n",
      "Attention, there is only one dataloader - train\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-08-21 00:00:22,882] \n",
      "1/10 * Epoch 1 (train): _timer/_fps=5410.9082 | _timer/batch_time=0.0967 | _timer/data_time=0.0031 | _timer/model_time=0.0936 | loss=2.9761\n",
      "[2020-08-21 00:00:35,796] \n",
      "2/10 * Epoch 2 (train): _timer/_fps=5153.4843 | _timer/batch_time=0.1004 | _timer/data_time=0.0019 | _timer/model_time=0.0985 | loss=2.4049\n",
      "[2020-08-21 00:00:49,181] \n",
      "3/10 * Epoch 3 (train): _timer/_fps=4965.4075 | _timer/batch_time=0.1040 | _timer/data_time=0.0019 | _timer/model_time=0.1022 | loss=2.3017\n",
      "[2020-08-21 00:01:02,332] \n",
      "4/10 * Epoch 4 (train): _timer/_fps=5060.7588 | _timer/batch_time=0.1021 | _timer/data_time=0.0022 | _timer/model_time=0.0999 | loss=2.2500\n",
      "[2020-08-21 00:01:15,734] \n",
      "5/10 * Epoch 5 (train): _timer/_fps=4977.5494 | _timer/batch_time=0.1041 | _timer/data_time=0.0020 | _timer/model_time=0.1022 | loss=2.2175\n",
      "[2020-08-21 00:01:29,931] \n",
      "6/10 * Epoch 6 (train): _timer/_fps=4760.8442 | _timer/batch_time=0.1103 | _timer/data_time=0.0021 | _timer/model_time=0.1082 | loss=2.1933\n",
      "[2020-08-21 00:01:46,138] \n",
      "7/10 * Epoch 7 (train): _timer/_fps=4481.8839 | _timer/batch_time=0.1260 | _timer/data_time=0.0021 | _timer/model_time=0.1239 | loss=2.1734\n",
      "[2020-08-21 00:01:59,087] \n",
      "8/10 * Epoch 8 (train): _timer/_fps=5123.0435 | _timer/batch_time=0.1006 | _timer/data_time=0.0018 | _timer/model_time=0.0988 | loss=2.1494\n",
      "[2020-08-21 00:02:17,749] \n",
      "9/10 * Epoch 9 (train): _timer/_fps=4436.7098 | _timer/batch_time=0.1451 | _timer/data_time=0.0029 | _timer/model_time=0.1421 | loss=2.1381\n",
      "[2020-08-21 00:02:34,655] \n",
      "10/10 * Epoch 10 (train): _timer/_fps=4247.6580 | _timer/batch_time=0.1315 | _timer/data_time=0.0024 | _timer/model_time=0.1291 | loss=2.1250\n",
      "Top best models:\n",
      "logs/checkpoints/train.10.pth\t2.1250\n",
      "=> Loading checkpoint logs/checkpoints/best_full.pth\n",
      "loaded state checkpoint logs/checkpoints/best_full.pth (global epoch 10, epoch 10, stage train)\n"
     ]
    }
   ],
   "source": [
    "runner.train(\n",
    "    loaders={\"train\": train_loader}, \n",
    "    model=model,\n",
    "    optimizer=optimizer, \n",
    "    callbacks=callbacks,\n",
    "    num_epochs=10, \n",
    "    logdir=\"./logs\",\n",
    "    timeit=True,\n",
    "    load_best_on_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O God, O God! Thoughter to shead,\n",
      "I shall, that ther he till sor have thee digne, a to me that to the tatie,\n",
      "Those this to the sto the strought he my tin as seater, I shead\n",
      "Whiste he she sir the hee and to be a bard to that stentistill ster, the his a so the thes he had\n",
      "I have shall and ter and\n",
      "I comme som shade selve son to by son, sore some he deart teated so the art,\n",
      "I some his he to mus, sir, ting there heartin tis sees,\n",
      "To shou he the havorsell be a that to hand his\n",
      "Wath be heate then thim as to bright all her that be ther hat he self there thy and\n",
      "Is sir this to stay, my mucester the the my lett of and aly, a me.\n",
      "\n",
      "This he she a se the to be to be andes seak his and the my thise and to bungest and most astrath stere the herear bead, the shall hour he to stay besst son murs, I this the so teat to heeady til sintly ter sing as a thee dit a as a but, a and hat shere shere, I a his and tearth a the all she his there a to sit, sirs off your thy as ter hearss that to sice ond a statter's and morth me to sheere so be a souce this the strued sin stay, there ther, mading of tione.\n",
      "Whes my and mins the my loves the thath. I the shall thin he ans that ther here and\n",
      "I come and theent the thou spor so thord. A and to son sher ther son, to sing and and a me that and thes of\n",
      "Wer, the to me, sell'd and ther the mant thy.\n",
      "\n",
      "\n",
      "CLORKE VIOLAND:\n",
      "I to thereser:\n",
      "Tin to so strang a shall tand set the his this too be my love stion stany sins to teating, mand a me a to ther as art tents theresss fore he aster of the he sher ond my ster's and sour but by to be thouth see here he dith shall st ter she to by sting and so be my ther, I to sto me, ther he a too hou se a my sher of and, I hee shall there seeady to se to made sees fir there of that the sholed ther to shave, a are hus a many self,\n",
      "As and takes tall hereese of a shat hat by shing thand the sto hou bre a stise a and morted a ther the seeath thour hady mad and huse her to me.\n",
      "\n",
      "\n",
      "MIORS:\n",
      "And seetin as the should the seetionst on mading to he a bre \n"
     ]
    }
   ],
   "source": [
    "# alright, let's sample some character-level shakespear\n",
    "from mingpt.utils import sample\n",
    "\n",
    "context = \"O God, O God!\"\n",
    "x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None,...].to(runner.device)\n",
    "y = sample(model, x, 2000, temperature=0.9, sample=True, top_k=5)[0]\n",
    "completion = ''.join([train_dataset.itos[int(i)] for i in y])\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# well that was fun"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
